{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11941173,"sourceType":"datasetVersion","datasetId":7506944}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install efficientnet_pytorch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"install","metadata":{}},{"cell_type":"code","source":"# train.py\nimport os, cv2, torch, pandas as pd, numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms\nfrom efficientnet_pytorch import EfficientNet\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Configs\nDATA_DIR = \"/kaggle/input/soil-classification-1/soil_classification-2025\"\nTRAIN_IMG = os.path.join(DATA_DIR, \"train\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train_labels.csv\")\nOUTPUT_DIR = \"/kaggle/working\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nN_FOLDS, BATCH_SIZE, NUM_EPOCHS, NUM_CLASSES = 5, 32, 20, 4\nLR, WD = 3e-4, 1e-5\n\nlabel2idx = {'Alluvial soil': 0, 'Black Soil': 1, 'Clay soil': 2, 'Red soil': 3}\ndf = pd.read_csv(TRAIN_CSV)\ndf['soil_type_idx'] = df['soil_type'].map(label2idx)\n\n# Transforms\ntransform_train = transforms.Compose([\n    transforms.Resize((300, 300)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(0.2, 0.2, 0.2),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ntransform_val = transforms.Compose([\n    transforms.Resize((300, 300)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ndef load_clahe_image(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img, (300, 300))\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    cl = cv2.createCLAHE(2.0, (8,8)).apply(l)\n    return Image.fromarray(cv2.cvtColor(cv2.merge((cl, a, b)), cv2.COLOR_LAB2RGB))\n\nclass SoilDataset(Dataset):\n    def __init__(self, df, img_dir, transform):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        img = load_clahe_image(os.path.join(self.img_dir, row['image_id']))\n        return self.transform(img), row['soil_type_idx']\n\ndef train_one_fold(train_idx, val_idx, fold):\n    train_df, val_df = df.loc[train_idx], df.loc[val_idx]\n    weights = 1.0 / train_df['soil_type_idx'].value_counts().sort_index().values\n    sample_weights = train_df['soil_type_idx'].map(lambda x: weights[x]).values\n    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n\n    train_loader = DataLoader(SoilDataset(train_df, TRAIN_IMG, transform_train), batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)\n    val_loader = DataLoader(SoilDataset(val_df, TRAIN_IMG, transform_val), batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n    model = EfficientNet.from_pretrained('efficientnet-b3')\n    model._fc = nn.Linear(model._fc.in_features, NUM_CLASSES)\n    model.to(DEVICE)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n\n    best_min_f1 = 0.0\n    for epoch in range(1, NUM_EPOCHS + 1):\n        model.train()\n        train_losses = []\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            optimizer.zero_grad()\n            loss = criterion(model(imgs), labels)\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n        scheduler.step()\n\n        model.eval()\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n                preds = model(imgs).argmax(1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n        min_f1 = f1_score(all_labels, all_preds, average=None).min()\n        print(f\"Fold {fold} Epoch {epoch}: Loss {np.mean(train_losses):.4f}, Min F1 {min_f1:.4f}\")\n        if min_f1 > best_min_f1:\n            best_min_f1 = min_f1\n            torch.save(model.state_dict(), f\"{OUTPUT_DIR}/model_fold{fold}.pth\")\n\n# K-Fold\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df, df['soil_type_idx'])):\n    print(f\"\\n=== Fold {fold} ===\")\n    train_one_fold(train_idx, val_idx, fold)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"# inference.py\nimport os, cv2, torch, pandas as pd, numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom efficientnet_pytorch import EfficientNet\n\n# Configs\nDATA_DIR = \"/kaggle/input/soil-classification-1/soil_classification-2025\"\nTEST_IMG = os.path.join(DATA_DIR, \"test\")\nTEST_IDS = os.path.join(DATA_DIR, \"test_ids.csv\")\nOUTPUT_DIR = \"/kaggle/working\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE, NUM_CLASSES, N_FOLDS = 32, 4, 5\nidx2label = {0: 'Alluvial soil', 1: 'Black Soil', 2: 'Clay soil', 3: 'Red soil'}\n\ndef load_clahe_image(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img, (300, 300))\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    cl = cv2.createCLAHE(2.0, (8,8)).apply(l)\n    return Image.fromarray(cv2.cvtColor(cv2.merge((cl, a, b)), cv2.COLOR_LAB2RGB))\n\ntransform_val = transforms.Compose([\n    transforms.Resize((300, 300)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nclass TestDataset(Dataset):\n    def __init__(self, image_ids, img_dir, transform):\n        self.image_ids, self.img_dir, self.transform = image_ids, img_dir, transform\n    def __len__(self): return len(self.image_ids)\n    def __getitem__(self, idx):\n        img_id = self.image_ids[idx]\n        img = load_clahe_image(os.path.join(self.img_dir, img_id))\n        return self.transform(img), img_id\n\ntest_ids = pd.read_csv(TEST_IDS)\ntest_list = test_ids['image_id'].tolist()\ntest_loader = DataLoader(TestDataset(test_list, TEST_IMG, transform_val), batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nensemble_probs = {img_id: [] for img_id in test_list}\nfor fold in range(N_FOLDS):\n    model = EfficientNet.from_name('efficientnet-b3')\n    model._fc = torch.nn.Linear(model._fc.in_features, NUM_CLASSES)\n    model.load_state_dict(torch.load(f\"{OUTPUT_DIR}/model_fold{fold}.pth\"))\n    model = model.to(DEVICE).eval()\n\n    with torch.no_grad():\n        for imgs, img_ids in test_loader:\n            imgs = imgs.to(DEVICE)\n            probs = model(imgs).cpu().softmax(1).numpy()\n            for i in range(len(img_ids)):\n                ensemble_probs[img_ids[i]].append(probs[i])\n\n# Write submission\nout = []\nfor img_id in test_list:\n    avg = np.mean(ensemble_probs[img_id], axis=0)\n    out.append({'image_id': img_id, 'soil_type': idx2label[int(avg.argmax())]})\n\npd.DataFrame(out).to_csv(f\"{OUTPUT_DIR}/submission.csv\", index=False)\nprint(\"submission.csv written\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"inference","metadata":{}}]}